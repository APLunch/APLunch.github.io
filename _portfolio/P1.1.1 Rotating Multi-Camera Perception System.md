---
title: "Rotating Multi-Camera Perception System"
excerpt: "A dynamically calibrated, behavior-tree–integrated rotating camera pole enabling full 360° coverage, safer manipulation, and 15% throughput gains. <br/><img src='/images/rotating_pole.gif' width=500>"
collection: portfolio
---

<img src='/images/rotating_pole.gif' width=600>

<img src='/images/rotating_pole_auto_calibration.gif' width=600>

## Objective
Pixmo’s manipulation workspace contains significant blind spots due to the robot’s geometry, mounted accessories, and customer-specific environments. These occlusions limit perception, reduce planner confidence, and prevent aggressive but safe motion profiles.  
The goal of the camera-pole project was to create a **full-workspace, continuously updated 3D perception system**—built on a custom rotating camera pole—providing complete visibility around the robot and enabling safer, faster, and more intelligent behavior planning.

## Technical Summary

### Hardware + System Architecture
A compact motorized camera pole was co-designed with the hardware team and integrated into Pixmo’s sensor stack. The pole rotates multiple calibrated RGB-D camera around the robot, capturing dense 3D observations across the full 360° workspace.

### Calibration Pipeline (Non-Linear Optimization)
I designed and implemented a **two-stage extrinsic calibration pipeline**:
1. **Initial geometric circle-fit estimation** using tracked calibration-board trajectories.  
2. **Levenberg–Marquardt non-linear optimization** over all frames (bundle-adjustment style) solving for:
   - Camera-to-pole transform  
   - Pole axis orientation  
   - Rotation radius + phase  
   - Pole-to-base transform

This produced a high-accuracy model of the camera’s motion, enabling milimeter-level global alignment of every frame.

### Real-Time Perception Stack
On top of the calibrated model, I built the perception system that:
- Reconstructs a **globally aligned 3D panoramic map** as the pole rotates.  
- Generates dense geometry for planner collision checking.  
- Identifies pallet, wall, and conveyor obstacles.  
- Provides continuous spatial updates for motion planning and safety monitors.

### Behavior Tree Integration
The entire pipeline was integrated into Pixmo’s **Behavior Tree (BT)** framework:
- BT nodes trigger camera-pole sweeps during critical behaviors (approach, pick, reorientation, stow).  
- Perception results feed directly into task-level decision logic.  
- Failure handling and fallback states leverage real-time scene updates from the pole.

### Simulation Support for Off-Hardware Development
I built a camera-pole simulation model for our internal dev stack:
- Programmable virtual pole kinematics  
- Synthetic depth + RGB data generation  
- Drop-in replacement for real hardware streams  

This allowed rapid iteration of BT integration, and perception logic **without requiring the physical robot**, accelerating development and reducing hardware downtime.

## Results

- **15% increase in overall system throughput**, enabled by improved planner confidence and reduction in blind-zone slow-downs.  
- **Significantly safer motion execution** due to continuous clearance estimation from dynamic camera sweeps.  
- **Higher task success rates** in cluttered or previously unobservable regions.  
- **Robust simulation-to-real transfer**, shortening development cycles.  
- The camera-pole perception module is now a standard part of Pixmo’s core perception stack.

## Detailed Documentation
*Restricted by non-disclosure agreements.*

